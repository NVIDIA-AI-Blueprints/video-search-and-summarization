#Use latest public image 2.3.0.
VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.3.0

#Adjust ports if needed
FRONTEND_PORT=9101
BACKEND_PORT=8100

#Change default user and pass if needed
GRAPH_DB_USERNAME=neo4j
GRAPH_DB_PASSWORD=password

#Update paths local paths to config files if needed
CA_RAG_CONFIG=./config.yaml
GUARDRAILS_CONFIG=./guardrails

#Adjust misc configs if needed
DISABLE_GUARDRAILS=true
#For H100 Deployment
NVIDIA_VISIBLE_DEVICES=0
#For L40S Deployment
#NVIDIA_VISIBLE_DEVICES=0,1,2 

#If using NIM(s) locally, specify the local NIM cache
LOCAL_NIM_CACHE=/tmp

### LOCAL_DEPLOYMENT: Set VLM to VILA ###
VLM_MODEL_TO_USE=vila-1.5
MODEL_PATH=ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8

### LOCAL_DEPLOYMENT_SINGLE_GPU: Set VLM to NVILA ###
#VLM_MODEL_TO_USE=nvila 
#MODEL_PATH=git:https://huggingface.co/Efficient-Large-Model/NVILA-15B
#NVILA_VIDEO_MAX_TILES=1

### REMOTE_LLM_DEPLOYMENT: Set VLM to VILA ### 
#NGC_API_KEY=abc123*** #api key to pull model from NGC. Add this if you are not using the jupyter notebooks
#ENABLE_AUDIO=false
#RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
#RIVA_ASR_GRPC_PORT=443
#RIVA_ASR_SERVER_IS_NIM=true
#RIVA_ASR_SERVER_USE_SSL=true
#RIVA_ASR_SERVER_API_KEY=nvapi-*** #FIXME - api key RIVA ASR server
#RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"
#DISABLE_CV_PIPELINE=true
#VLM_MODEL_TO_USE=vila-1.5 #Requires at least 80GB of VRAM to build the TRT engine. Minimum GPU requirement is 2xL40S, 1xA100 80GB, 1xH100, 1xH200.
#MODEL_PATH=ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8

### REMOTE_VLM_DEPLOYMENT: Set VLM to OpenAI model ### 
#OPENAI_API_KEY=sk-*** #FIXME - api key for the remote VLM 
#NGC_API_KEY=abc123*** #api key to pull model from NGC. Add this if you are not using the jupyter notebooks
#ENABLE_AUDIO=false
#DISABLE_CV_PIPELINE=true
#RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
#RIVA_ASR_GRPC_PORT=443
#RIVA_ASR_SERVER_IS_NIM=true
#RIVA_ASR_SERVER_USE_SSL=true
#RIVA_ASR_SERVER_API_KEY=nvapi-***
#RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"
#VLM_MODEL_TO_USE=openai-compat
#VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME=gpt-4o #FIX ME - change VLM model on remote endpoint
#VIA_VLM_ENDPOINT=https://api.openai.com/v1/chat/completions #FIX ME - change url to point to remote VLM endpoint. Can be any VLM with an openAI compatible API.

