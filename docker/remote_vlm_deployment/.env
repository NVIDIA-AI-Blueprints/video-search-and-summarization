NVIDIA_API_KEY=nvapi-*** #FIXME - api key to access NIM endpoints. Should come from build.nvidia.com
OPENAI_API_KEY=sk-*** #FIXME - api key for the remote VLM 

#Use latest public image 2.2.0.
VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.2.0

#Adjust ports if needed
FRONTEND_PORT = 9100
BACKEND_PORT = 8100

#Change default user and pass if needed 
GRAPH_DB_USERNAME=neo4j
GRAPH_DB_PASSWORD=password 

#Update paths local paths to config files if needed. If it appears VSS is not using these configurations, then change the relative paths to absolute paths. 
CA_RAG_CONFIG=./config.yaml 
GUARDRAILS_CONFIG=./guardrails

#Set VLM to OpenAI model 
VLM_MODEL_TO_USE=openai-compat 
VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME=gpt-4o #FIX ME - change VLM model on remote endpoint 
VIA_VLM_ENDPOINT=https://api.openai.com/v1/chat/completions #FIX ME - change url to point to remote VLM endpoint. Can be any VLM with an openAI compatible API. 

#Adjust misc configs if needed
DISABLE_GUARDRAILS=false
NVIDIA_VISIBLE_DEVICES=0 #If you system has more than 1 GPU, configure which GPUs to use to run VSS. VSS can utilize multiple GPUs for lower latency summarization. 
