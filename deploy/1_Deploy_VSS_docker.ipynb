{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA AI Blueprint for Video Search and Summarization: Docker Deployment\n",
    "\n",
    "This notebook will go over the steps to deploy the NVIDIA AI Blueprint for Video Search and Summarization which can ingest massive volumes of live or archived videos and extract insights for summarization and interactive Q&A.\n",
    "\n",
    "We will go over the docker compose deployment steps which as an alternative to the helm chart deployment. As this blueprint uses multiple models including a VLM, LLM, Reranker, Embedding model, there are multiple ways to deploy the blueprint based on the model deployment (Self-hosted/NV-hosted /w API Key). Here we will be running all models locally/self-hosted.\n",
    "\n",
    "**Note**: this notebook is designed to run as a [brev.dev launchable](https://console.brev.dev/launchable/deploy/?launchableID=env-2olGFXbhH0qEtU47MJ4tpxCAZIn) on 8XL40S, 8xA100 or 8xH100 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Verify and Update Driver and CUDA version to be the following:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  Skip this step if you already have the right driver version. Please ensure you add the NGC_API_KEY after the reboot.\n",
    " </div>\n",
    " \n",
    "\n",
    "Ensure that the driver and cuda versions matches the ones below. \n",
    "- Driver Version: 535.x.x\n",
    "- CUDA Version 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install nvidia-driver-535 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reboot the system\n",
    "\n",
    "System needs to be rebooted after driver update. After running the following cell, wait for a few seconds and reload the jupyter terminal. Then verify new driver version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo reboot now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check driver version after reboot\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain NVIDIA API Keys\n",
    "\n",
    "This key will be used to pull relevant models and containers from build.nvidia.com and NGC.\n",
    "\n",
    "Generate the key from [NGC Portal](https://ngc.nvidia.com/) using the same account used to apply for blueprint Early Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"***\" #Replace with your key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the user is on right path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVIDIA-AI-Blueprints/video-search-and-summarization.git\n",
    "# %cd video-search-and-summarization/docker/launchables\n",
    "\n",
    "%cd ../docker/launchables\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment: Using all self-hosted models\n",
    "\n",
    "We will be using Cosmos Nemotron VLM, which is part of the main container. All other models need to be set up before proceeding with the blueprint container. These include:\n",
    "- [Embedding NIM](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2)\n",
    "- [Reranker NIM](https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2)\n",
    "- [LLM NIM](https://build.nvidia.com/meta/llama-3_1-70b-instruct)\n",
    "\n",
    "\n",
    "### GPU Configuration\n",
    "\n",
    "![VSS GPU Layout](images/vss_gpu_layout.png)\n",
    "\n",
    "In order to update the GPUs used by each model:\n",
    "- **LLM, Embedding and Reranking models:** Update the ```--gpus``` parameters while deploying NIMs in Steps 2-4 below.\n",
    "- **VLM:** Update ```NVIDIA_VISIBLE_DEVICES``` in docker/.env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Export Necessary Parameters and login to Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"~/.cache/nim\")\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Launch the Reranker NIM. \n",
    "\n",
    "Here we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=4\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9235:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Launch the Embedding NIM.\n",
    "\n",
    "Here we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "    --gpus '\"device=5\"' \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 9234:8000 \\\n",
    "    -d \\\n",
    "    nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Launch the LLM NIM.\n",
    "Note: If you're logged in as root, make a separate llm_user account and give it permission to the nim cache folder.\n",
    "\n",
    "Again, we have preset the GPUs to use ```--gpus``` and port ```-p``` for the most optimal deployment on 8xL40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    "  --gpus '\"device=0,1,2,3\"' \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NGC_API_KEY=$NGC_API_KEY \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  nvcr.io/nim/meta/llama-3.1-70b-instruct:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify all 3 NIMs are running\n",
    "\n",
    "After running the following cell, you should be able to see three containers, one for each model.\n",
    "\n",
    "![Container List](images/containers_pre_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take a couple of miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests\n",
    "\n",
    "url = 'http://localhost:8000/v1/health/ready' #make sure the LLM NIM port is correct\n",
    "headers = {'accept': 'application/json'}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get(\"message\") == \"Service is ready.\":\n",
    "                print(\"Service is ready.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "        else:\n",
    "            print(f\"Unexpected status code {response.status_code}. Waiting for 30 seconds...\")\n",
    "    except requests.ConnectionError:\n",
    "        print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Launch the Blueprint\n",
    "\n",
    "Before proceeding, make sure you have compose.yaml, config.yaml and .env in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check docker compose version (recommended v2.32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p ~/.docker/cli-plugins\n",
    "#!curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n",
    "#!chmod +x ~/.docker/cli-plugins/docker-compose\n",
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Docker Compose Deployment**\n",
    "\n",
    "Here, we start the docker container to spin up the blueprint. The container performs the following main steps:\n",
    "1. Downloads the VLM\n",
    "2. Performs model calibration\n",
    "3. Generates a TRT-LLM Engine\n",
    "4. Spins up the Milvus and Neo4J database.\n",
    "5. Starts Video Search and Summarization service\n",
    "6. Finally, we get the frontend and backend endpoints\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  This step can take around 30 minutes for the first run as it goes through Steps 1-3 mentioned above\n",
    " </div>\n",
    "\n",
    "The below code cell filters out important logs so that the output is not very verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!docker compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "keywords = [\"Milvus server started\", \"Starting quantization\", \"Quantization done\", \n",
    "            \"Engine generation completed\", \"TRT engines generated\", \"Uvicorn\", \"VIA Server loaded\", \"Backend\", \"Frontend\"]\n",
    "\n",
    "# Start the docker compose process in detached mode\n",
    "subprocess.run(['docker', 'compose', 'up', '--quiet-pull', '-d'])\n",
    "\n",
    "def filter_logs(logs, keywords):\n",
    "    return [line for line in logs.splitlines() if any(keyword in line for keyword in keywords)]\n",
    "\n",
    "printed_lines = set()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        logs = subprocess.check_output(['docker', 'compose', 'logs', '--no-color'], universal_newlines=True)\n",
    "        filtered_logs = filter_logs(logs, keywords)\n",
    "        new_logs = [line for line in filtered_logs if line not in printed_lines]\n",
    "        \n",
    "        for line in new_logs:\n",
    "            print(line)\n",
    "            printed_lines.add(line)\n",
    "            if \"Frontend\" in line:\n",
    "                print(\"VSS Server ran successfully.\")\n",
    "                print(\"Add frontend port to the tunnel on Brev Portal to access UI.\")\n",
    "                raise SystemExit\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping log tailing...\")\n",
    "except SystemExit:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, run this command if you want to see a complete log trail on the notebook\n",
    "#!docker compose up  --quiet-pull  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Access instance on VSCode and add ports\n",
    "\n",
    "1. First, go to the \"Access\" tab  \n",
    "![Access Tab](./images/brev_access_tab.png)\n",
    "\n",
    "2. Refer to the commands in \"Using Brev CLI\" to open VSCode locally  \n",
    "\n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  Make sure to <a href=\"https://code.visualstudio.com/docs/setup/mac#_configure-the-path-with-vs-code\" target=\"_blank\">Configure the path with VS Code</a> if you run into erros while accessing instance through VSCode.\n",
    "    </div>  \n",
    "\n",
    "    ![Access Tab](images/brev_vscode_access.png)\n",
    "\n",
    "\n",
    "3. Navigate to the Ports view in the Panel region (Ports: Focus on Ports View), and select Forward a Port  \n",
    "![VSCode Ports](images/vscode_ports.png) \n",
    "\n",
    "4. Add frontend (9100) and backend (8100) ports, and access blueprint UI by clicking on the \"localhost:9100\"  \n",
    "![VSCode Ports](images/vscode_access_vss_ui.png) \n",
    "\n",
    "\n",
    "\n",
    "### [Optional] Add port on Brev to access the UI\n",
    "\n",
    "1. Scroll down to the \"Using Tunnels\" section and click on \"Share a Service\"    \n",
    "![Access Tab](images/brev_tunnels_section.png)\n",
    "\n",
    "2. Now, add port \"9100\". This is set in .env file which is configurable   \n",
    "![Access Tab](images/brev_add_port.png) \n",
    "\n",
    "3. Click on the new Sharable URL link to access blueprint UI.\n",
    "\n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        <b>Note:</b>  The Shareable URL could take 5-10 mins to get active, please reopen the link in some time if you face issues.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uninstalling the blueprint\n",
    "\n",
    "Uncomment the following cell to stop the blueprint container, followed by stopping all model containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # To bring down the blueprint instance\n",
    "# !docker compose down\n",
    "\n",
    "# # To stop all other containers\n",
    "# !docker stop $(docker ps -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
